{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1035,"status":"ok","timestamp":1701754405155,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"ycyen8eOJ3Xp"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.models import Sequential\n","from google.colab import drive\n","from tensorflow.keras.applications import MobileNet"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1840,"status":"ok","timestamp":1701754408088,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"QdVzxUwJBft5"},"outputs":[],"source":["import re\n","import subprocess\n","from pathlib import Path\n","from typing import List, Optional\n","\n","import openvino as ov\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tqdm import tqdm\n","\n","import nncf\n","\n","import os\n","import random\n","import cv2\n","import torch\n","import numpy as np\n","from torchvision import transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3168,"status":"ok","timestamp":1701754453397,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"e4GyyT0ZKgjT","outputId":"c54239ae-a999-4086-d805-59c4d301d94a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# 코랩 파일 마운트\n","drive.mount('/content/drive')\n","\n","def loadfile(path):\n","    X = []\n","    Y = []\n","\n","    for label in ('danger_key', 'danger', 'danger_st', 'danger_wi','normal' ,'warnning_both','warnning_leaf','warnning_sq') :\n","        print(\"Loading training images for the label: \" + label)\n","\n","        for filename in os.listdir(path + label + \"/\"):\n","            img = cv2.imread(path + label + \"/\" + filename)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n","            img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n","            img = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n","            img = cv2.resize(img, (224, 224))\n","            X.append(img)\n","            if label == 'danger':\n","                Y.append(0)\n","            elif label == 'danger_key':\n","                Y.append(1)\n","            elif label == 'danger_st':\n","                Y.append(2)\n","            elif label == 'danger_wi':\n","                Y.append(3)\n","            elif label == 'normal':\n","                Y.append(4)\n","            elif label == 'warnning_sq':\n","                Y.append(5)\n","            elif label == 'warnning_leaf':\n","                Y.append(6)\n","            elif label == 'warnning_both':\n","                Y.append(7)\n","\n","\n","    X = np.array(X)\n","    Y = np.array(Y)\n","\n","    return X, Y\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46556,"status":"ok","timestamp":1701754502619,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"qGa10n4nLJEh","outputId":"26f20b40-02c2-4e15-ce16-b6f852c3a0c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading training images for the label: danger_key\n","Loading training images for the label: danger\n","Loading training images for the label: danger_st\n","Loading training images for the label: danger_wi\n","Loading training images for the label: normal\n","Loading training images for the label: warnning_both\n","Loading training images for the label: warnning_leaf\n","Loading training images for the label: warnning_sq\n"]}],"source":["# 데이터 불러오기\n","X, y = loadfile('/content/drive/MyDrive/data/')\n","\n","# Train :test : val = 6:2:2 / set 분리\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Normalize data\n","X_train = X_train.astype('float32') / 255\n","X_test = X_test.astype('float32') / 255\n","X_val = X_val.astype('float32') / 255\n","\n","# Augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True)\n","\n","# Compute quantities required for featurewise normalization\n","datagen.fit(X_train)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4938,"status":"ok","timestamp":1701754958715,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"7M8JcdtBL6xW"},"outputs":[],"source":["from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n","from tensorflow.keras.regularizers import l1_l2\n","\n","# 기존 Mobilenet V2 모델 불러오기 (include_top=False는 완전 연결 계층을 포함할지 여부를 결정합니다)\n","\n","# 새로운 모델 정의하기\n","# Build model\n","model = Sequential()\n","model.add(MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet'))  # Use MobileNet\n","model.add(GlobalAveragePooling2D())\n","model.add(Dropout(0.5))  # Increase dropout\n","model.add(Dense(512, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))  # Add additional Dense layer, Add L1 and L2 regularization\n","model.add(BatchNormalization())\n","model.add(Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))   # Add additional Dense layer, Add L1 and L2 regularization\n","model.add(BatchNormalization())\n","model.add(Dense(8, activation='softmax')) # Class 조정 3 -\u003e 8\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":717,"status":"ok","timestamp":1701754964220,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"UasQe2ANM_S6","outputId":"062e3047-f63c-4c06-fe7c-436219c8d375"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n"," tional)                                                         \n","                                                                 \n"," global_average_pooling2d_3  (None, 1280)              0         \n","  (GlobalAveragePooling2D)                                       \n","                                                                 \n"," dropout_1 (Dropout)         (None, 1280)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               655872    \n","                                                                 \n"," batch_normalization (Batch  (None, 512)               2048      \n"," Normalization)                                                  \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 256)               1024      \n"," chNormalization)                                                \n","                                                                 \n"," dense_2 (Dense)             (None, 8)                 2056      \n","                                                                 \n","=================================================================\n","Total params: 3050312 (11.64 MB)\n","Trainable params: 3014664 (11.50 MB)\n","Non-trainable params: 35648 (139.25 KB)\n","_________________________________________________________________\n"]}],"source":["# Model summary\n","model.summary()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537442,"status":"ok","timestamp":1701767524662,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"fUjvISTzNCE7","outputId":"7bfeb05e-7e7e-4be2-f621-182e775ff4d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","16/16 [==============================] - 142s 9s/step - loss: 219.5138 - accuracy: 0.2296 - val_loss: 185.0252 - val_accuracy: 0.0819\n","Epoch 2/100\n","16/16 [==============================] - 103s 6s/step - loss: 157.0576 - accuracy: 0.4650 - val_loss: 130.5753 - val_accuracy: 0.2515\n","Epoch 3/100\n","16/16 [==============================] - 105s 7s/step - loss: 109.0652 - accuracy: 0.4747 - val_loss: 90.7246 - val_accuracy: 0.2690\n","Epoch 4/100\n","16/16 [==============================] - 110s 7s/step - loss: 76.1650 - accuracy: 0.5720 - val_loss: 63.9370 - val_accuracy: 0.2982\n","Epoch 5/100\n","16/16 [==============================] - 107s 7s/step - loss: 54.4574 - accuracy: 0.5992 - val_loss: 47.2644 - val_accuracy: 0.3450\n","Epoch 6/100\n","16/16 [==============================] - 103s 6s/step - loss: 40.8396 - accuracy: 0.6498 - val_loss: 36.4382 - val_accuracy: 0.3509\n","Epoch 7/100\n","16/16 [==============================] - 100s 6s/step - loss: 32.7302 - accuracy: 0.6498 - val_loss: 30.7107 - val_accuracy: 0.1404\n","Epoch 8/100\n","16/16 [==============================] - 102s 6s/step - loss: 26.3888 - accuracy: 0.6868 - val_loss: 24.4018 - val_accuracy: 0.4327\n","Epoch 9/100\n","16/16 [==============================] - 100s 6s/step - loss: 21.4155 - accuracy: 0.7121 - val_loss: 20.0989 - val_accuracy: 0.4035\n","Epoch 10/100\n","16/16 [==============================] - 106s 7s/step - loss: 19.1815 - accuracy: 0.7374 - val_loss: 20.0289 - val_accuracy: 0.3509\n","Epoch 11/100\n","16/16 [==============================] - 96s 6s/step - loss: 17.7384 - accuracy: 0.7510 - val_loss: 17.2528 - val_accuracy: 0.2807\n","Epoch 12/100\n","16/16 [==============================] - 98s 6s/step - loss: 15.2597 - accuracy: 0.7724 - val_loss: 15.6344 - val_accuracy: 0.3450\n","Epoch 13/100\n","16/16 [==============================] - 99s 6s/step - loss: 14.0541 - accuracy: 0.7899 - val_loss: 15.0030 - val_accuracy: 0.2865\n","Epoch 14/100\n","16/16 [==============================] - 98s 6s/step - loss: 13.2591 - accuracy: 0.7957 - val_loss: 13.2538 - val_accuracy: 0.5380\n","Epoch 15/100\n","16/16 [==============================] - 101s 6s/step - loss: 11.4169 - accuracy: 0.8191 - val_loss: 11.5970 - val_accuracy: 0.5556\n","Epoch 16/100\n","16/16 [==============================] - 98s 6s/step - loss: 10.0746 - accuracy: 0.8346 - val_loss: 10.8524 - val_accuracy: 0.2515\n","Epoch 17/100\n","16/16 [==============================] - 101s 6s/step - loss: 13.8002 - accuracy: 0.8074 - val_loss: 17.7530 - val_accuracy: 0.2573\n","Epoch 18/100\n","16/16 [==============================] - 101s 6s/step - loss: 13.4769 - accuracy: 0.7802 - val_loss: 13.5489 - val_accuracy: 0.3275\n","Epoch 19/100\n","16/16 [==============================] - 102s 6s/step - loss: 12.7281 - accuracy: 0.7685 - val_loss: 14.1108 - val_accuracy: 0.4152\n","Epoch 20/100\n","16/16 [==============================] - 100s 7s/step - loss: 11.0829 - accuracy: 0.8016 - val_loss: 11.0695 - val_accuracy: 0.2456\n","Epoch 21/100\n","16/16 [==============================] - 99s 6s/step - loss: 8.5355 - accuracy: 0.8171 - val_loss: 9.1050 - val_accuracy: 0.3275\n","Epoch 22/100\n","16/16 [==============================] - 99s 6s/step - loss: 7.2466 - accuracy: 0.8560 - val_loss: 8.4648 - val_accuracy: 0.1988\n","Epoch 23/100\n","16/16 [==============================] - 99s 6s/step - loss: 8.3275 - accuracy: 0.8035 - val_loss: 18.1660 - val_accuracy: 0.2105\n","Epoch 24/100\n","16/16 [==============================] - 98s 6s/step - loss: 10.2939 - accuracy: 0.7510 - val_loss: 15.5864 - val_accuracy: 0.1696\n","Epoch 25/100\n","16/16 [==============================] - 113s 7s/step - loss: 10.8779 - accuracy: 0.7471 - val_loss: 18.8785 - val_accuracy: 0.1520\n","Epoch 26/100\n","16/16 [==============================] - 98s 6s/step - loss: 10.5320 - accuracy: 0.7840 - val_loss: 13.4453 - val_accuracy: 0.1988\n","Epoch 27/100\n","16/16 [==============================] - 103s 6s/step - loss: 8.5188 - accuracy: 0.8385 - val_loss: 11.2916 - val_accuracy: 0.1754\n","Epoch 28/100\n","16/16 [==============================] - 105s 7s/step - loss: 7.1212 - accuracy: 0.8346 - val_loss: 12.9593 - val_accuracy: 0.0526\n","Epoch 29/100\n","16/16 [==============================] - 101s 6s/step - loss: 7.3855 - accuracy: 0.8482 - val_loss: 11.4732 - val_accuracy: 0.2339\n","Epoch 30/100\n","16/16 [==============================] - 101s 6s/step - loss: 6.2598 - accuracy: 0.8774 - val_loss: 7.0513 - val_accuracy: 0.5146\n","Epoch 31/100\n","16/16 [==============================] - 105s 7s/step - loss: 6.3222 - accuracy: 0.8755 - val_loss: 8.8353 - val_accuracy: 0.4035\n","Epoch 32/100\n","16/16 [==============================] - 96s 6s/step - loss: 9.5710 - accuracy: 0.6926 - val_loss: 13.1514 - val_accuracy: 0.3567\n","Epoch 33/100\n","16/16 [==============================] - 98s 6s/step - loss: 10.4505 - accuracy: 0.6984 - val_loss: 11.9101 - val_accuracy: 0.2982\n","Epoch 34/100\n","16/16 [==============================] - 98s 6s/step - loss: 8.0775 - accuracy: 0.7899 - val_loss: 9.5359 - val_accuracy: 0.2515\n","Epoch 35/100\n","16/16 [==============================] - 103s 6s/step - loss: 8.4924 - accuracy: 0.7062 - val_loss: 22.4220 - val_accuracy: 0.0526\n","Epoch 36/100\n","16/16 [==============================] - 101s 6s/step - loss: 7.9436 - accuracy: 0.7490 - val_loss: 26.0703 - val_accuracy: 0.1404\n","Epoch 37/100\n","16/16 [==============================] - 96s 6s/step - loss: 6.8124 - accuracy: 0.7743 - val_loss: 14.7091 - val_accuracy: 0.0994\n","Epoch 38/100\n","16/16 [==============================] - 99s 6s/step - loss: 5.8132 - accuracy: 0.7860 - val_loss: 9.9607 - val_accuracy: 0.1754\n","Epoch 39/100\n","16/16 [==============================] - 97s 6s/step - loss: 5.3854 - accuracy: 0.8405 - val_loss: 16.0722 - val_accuracy: 0.0526\n","Epoch 40/100\n","16/16 [==============================] - 103s 6s/step - loss: 5.6764 - accuracy: 0.8482 - val_loss: 10.0535 - val_accuracy: 0.2105\n","Epoch 41/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.8406 - accuracy: 0.8755 - val_loss: 11.0623 - val_accuracy: 0.1754\n","Epoch 42/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.9283 - accuracy: 0.8463 - val_loss: 7.7624 - val_accuracy: 0.2632\n","Epoch 43/100\n","16/16 [==============================] - 103s 6s/step - loss: 5.5632 - accuracy: 0.8346 - val_loss: 19.9075 - val_accuracy: 0.0936\n","Epoch 44/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.9805 - accuracy: 0.8774 - val_loss: 10.6293 - val_accuracy: 0.3216\n","Epoch 45/100\n","16/16 [==============================] - 95s 6s/step - loss: 5.3578 - accuracy: 0.8911 - val_loss: 8.0748 - val_accuracy: 0.3216\n","Epoch 46/100\n","16/16 [==============================] - 104s 7s/step - loss: 4.6432 - accuracy: 0.8658 - val_loss: 9.0369 - val_accuracy: 0.2515\n","Epoch 47/100\n","16/16 [==============================] - 100s 6s/step - loss: 4.3968 - accuracy: 0.8813 - val_loss: 9.7389 - val_accuracy: 0.2164\n","Epoch 48/100\n","16/16 [==============================] - 100s 6s/step - loss: 4.9758 - accuracy: 0.8482 - val_loss: 8.3698 - val_accuracy: 0.3860\n","Epoch 49/100\n","16/16 [==============================] - 99s 6s/step - loss: 4.3527 - accuracy: 0.9125 - val_loss: 4.8115 - val_accuracy: 0.5614\n","Epoch 50/100\n","16/16 [==============================] - 97s 6s/step - loss: 4.5334 - accuracy: 0.8774 - val_loss: 7.7591 - val_accuracy: 0.3743\n","Epoch 51/100\n","16/16 [==============================] - 97s 6s/step - loss: 4.0805 - accuracy: 0.8891 - val_loss: 17.8868 - val_accuracy: 0.0819\n","Epoch 52/100\n","16/16 [==============================] - 99s 6s/step - loss: 5.4546 - accuracy: 0.7296 - val_loss: 20.8400 - val_accuracy: 0.1287\n","Epoch 53/100\n","16/16 [==============================] - 99s 6s/step - loss: 7.0902 - accuracy: 0.6323 - val_loss: 29.0456 - val_accuracy: 0.1520\n","Epoch 54/100\n","16/16 [==============================] - 97s 6s/step - loss: 6.3099 - accuracy: 0.7101 - val_loss: 18.1997 - val_accuracy: 0.1520\n","Epoch 55/100\n","16/16 [==============================] - 101s 6s/step - loss: 5.9733 - accuracy: 0.7646 - val_loss: 11.3604 - val_accuracy: 0.1696\n","Epoch 56/100\n","16/16 [==============================] - 100s 6s/step - loss: 6.6592 - accuracy: 0.6732 - val_loss: 10.8262 - val_accuracy: 0.0760\n","Epoch 57/100\n","16/16 [==============================] - 104s 7s/step - loss: 6.3829 - accuracy: 0.7198 - val_loss: 7.2359 - val_accuracy: 0.1696\n","Epoch 58/100\n","16/16 [==============================] - 95s 6s/step - loss: 5.2609 - accuracy: 0.7588 - val_loss: 6.0842 - val_accuracy: 0.1520\n","Epoch 59/100\n","16/16 [==============================] - 104s 7s/step - loss: 5.5497 - accuracy: 0.7996 - val_loss: 7.6048 - val_accuracy: 0.1637\n","Epoch 60/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.5790 - accuracy: 0.8327 - val_loss: 7.9956 - val_accuracy: 0.2339\n","Epoch 61/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.1977 - accuracy: 0.8230 - val_loss: 7.0003 - val_accuracy: 0.2982\n","Epoch 62/100\n","16/16 [==============================] - 103s 6s/step - loss: 4.3299 - accuracy: 0.8755 - val_loss: 6.7782 - val_accuracy: 0.2339\n","Epoch 63/100\n","16/16 [==============================] - 98s 6s/step - loss: 3.9771 - accuracy: 0.8385 - val_loss: 6.8513 - val_accuracy: 0.2515\n","Epoch 64/100\n","16/16 [==============================] - 102s 7s/step - loss: 3.7179 - accuracy: 0.8560 - val_loss: 8.1353 - val_accuracy: 0.1520\n","Epoch 65/100\n","16/16 [==============================] - 109s 7s/step - loss: 4.3642 - accuracy: 0.8230 - val_loss: 8.9743 - val_accuracy: 0.1520\n","Epoch 66/100\n","16/16 [==============================] - 110s 7s/step - loss: 3.7854 - accuracy: 0.8755 - val_loss: 6.6746 - val_accuracy: 0.2339\n","Epoch 67/100\n","16/16 [==============================] - 95s 6s/step - loss: 3.6441 - accuracy: 0.8891 - val_loss: 5.9526 - val_accuracy: 0.3392\n","Epoch 68/100\n","16/16 [==============================] - 97s 6s/step - loss: 3.7868 - accuracy: 0.8658 - val_loss: 6.4701 - val_accuracy: 0.3216\n","Epoch 69/100\n","16/16 [==============================] - 102s 6s/step - loss: 3.9064 - accuracy: 0.8716 - val_loss: 9.0512 - val_accuracy: 0.2222\n","Epoch 70/100\n","16/16 [==============================] - 95s 6s/step - loss: 4.9230 - accuracy: 0.8580 - val_loss: 12.7877 - val_accuracy: 0.2749\n","Epoch 71/100\n","16/16 [==============================] - 97s 6s/step - loss: 5.0908 - accuracy: 0.8813 - val_loss: 8.6150 - val_accuracy: 0.2398\n","Epoch 72/100\n","16/16 [==============================] - 98s 6s/step - loss: 4.4747 - accuracy: 0.8852 - val_loss: 11.1328 - val_accuracy: 0.1579\n","Epoch 73/100\n","16/16 [==============================] - 102s 6s/step - loss: 4.5639 - accuracy: 0.8716 - val_loss: 6.7677 - val_accuracy: 0.3099\n","Epoch 74/100\n","16/16 [==============================] - 101s 6s/step - loss: 6.8039 - accuracy: 0.8405 - val_loss: 15.6626 - val_accuracy: 0.0760\n","Epoch 75/100\n","16/16 [==============================] - 100s 6s/step - loss: 6.9392 - accuracy: 0.8774 - val_loss: 15.8971 - val_accuracy: 0.0702\n","Epoch 76/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.7651 - accuracy: 0.9027 - val_loss: 10.8716 - val_accuracy: 0.1813\n","Epoch 77/100\n","16/16 [==============================] - 100s 6s/step - loss: 4.7630 - accuracy: 0.9163 - val_loss: 7.2726 - val_accuracy: 0.2924\n","Epoch 78/100\n","16/16 [==============================] - 95s 6s/step - loss: 4.7304 - accuracy: 0.8774 - val_loss: 13.6122 - val_accuracy: 0.0409\n","Epoch 79/100\n","16/16 [==============================] - 97s 6s/step - loss: 5.5331 - accuracy: 0.8774 - val_loss: 11.6198 - val_accuracy: 0.1754\n","Epoch 80/100\n","16/16 [==============================] - 105s 6s/step - loss: 5.2992 - accuracy: 0.8638 - val_loss: 10.1337 - val_accuracy: 0.2456\n","Epoch 81/100\n","16/16 [==============================] - 100s 6s/step - loss: 5.2178 - accuracy: 0.8949 - val_loss: 13.8995 - val_accuracy: 0.0702\n","Epoch 82/100\n","16/16 [==============================] - 101s 6s/step - loss: 4.9960 - accuracy: 0.8969 - val_loss: 11.5732 - val_accuracy: 0.0877\n","Epoch 83/100\n","16/16 [==============================] - 103s 6s/step - loss: 4.5369 - accuracy: 0.9300 - val_loss: 11.8611 - val_accuracy: 0.1813\n","Epoch 84/100\n","16/16 [==============================] - 95s 6s/step - loss: 5.4111 - accuracy: 0.7996 - val_loss: 18.9646 - val_accuracy: 0.0643\n","Epoch 85/100\n","16/16 [==============================] - 95s 6s/step - loss: 4.8739 - accuracy: 0.8833 - val_loss: 12.1311 - val_accuracy: 0.1579\n","Epoch 86/100\n","16/16 [==============================] - 100s 6s/step - loss: 4.0989 - accuracy: 0.9125 - val_loss: 7.8625 - val_accuracy: 0.2398\n","Epoch 87/100\n","16/16 [==============================] - 97s 6s/step - loss: 3.5111 - accuracy: 0.9261 - val_loss: 7.3590 - val_accuracy: 0.1637\n","Epoch 88/100\n","16/16 [==============================] - 102s 6s/step - loss: 3.1725 - accuracy: 0.8988 - val_loss: 12.6059 - val_accuracy: 0.1520\n","Epoch 89/100\n","16/16 [==============================] - 105s 6s/step - loss: 3.3582 - accuracy: 0.9183 - val_loss: 9.3315 - val_accuracy: 0.1813\n","Epoch 90/100\n","16/16 [==============================] - 103s 6s/step - loss: 3.2632 - accuracy: 0.9202 - val_loss: 8.2606 - val_accuracy: 0.1579\n","Epoch 91/100\n","16/16 [==============================] - 99s 6s/step - loss: 2.8229 - accuracy: 0.9183 - val_loss: 5.7096 - val_accuracy: 0.2690\n","Epoch 92/100\n","16/16 [==============================] - 103s 6s/step - loss: 2.8488 - accuracy: 0.9125 - val_loss: 7.8179 - val_accuracy: 0.2398\n","Epoch 93/100\n","16/16 [==============================] - 95s 6s/step - loss: 3.5558 - accuracy: 0.9086 - val_loss: 11.5464 - val_accuracy: 0.2105\n","Epoch 94/100\n","16/16 [==============================] - 103s 6s/step - loss: 3.7638 - accuracy: 0.9202 - val_loss: 18.4755 - val_accuracy: 0.0468\n","Epoch 95/100\n","16/16 [==============================] - 105s 7s/step - loss: 4.8236 - accuracy: 0.8482 - val_loss: 10.9202 - val_accuracy: 0.2398\n","Epoch 96/100\n","16/16 [==============================] - 102s 6s/step - loss: 5.0638 - accuracy: 0.9144 - val_loss: 11.9312 - val_accuracy: 0.0585\n","Epoch 97/100\n","16/16 [==============================] - 97s 6s/step - loss: 5.2976 - accuracy: 0.9027 - val_loss: 20.4993 - val_accuracy: 0.0760\n","Epoch 98/100\n","16/16 [==============================] - 94s 6s/step - loss: 5.3891 - accuracy: 0.8794 - val_loss: 12.5971 - val_accuracy: 0.2281\n","Epoch 99/100\n","16/16 [==============================] - 93s 6s/step - loss: 4.8085 - accuracy: 0.8911 - val_loss: 11.0118 - val_accuracy: 0.3567\n","Epoch 100/100\n","16/16 [==============================] - 96s 6s/step - loss: 4.2833 - accuracy: 0.9163 - val_loss: 9.5471 - val_accuracy: 0.1696\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# Fit the model on the batches generated by datagen.flow().\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n","                    steps_per_epoch=len(X_train) / 32,\n","                    epochs=100,\n","                    validation_data=(X_val, y_val),\n","                    #callbacks=[early_stopping]\n","                    )\n","\n","model.save('/content/drive/MyDrive/231205_1430.h5')\n","tf.saved_model.save(model, '231205_1430')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"XsmXR-SKJUQn"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n","WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-23-5ba1e85affc7\u003e\", line 1, in \u003ccell line: 1\u003e\n","    tf.saved_model.save(model,'231205_1430')\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py\", line 1331, in save\n","    save_and_return_nodes(obj, export_dir, signatures, options)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py\", line 1373, in save_and_return_nodes\n","    path_helpers.get_or_create_variables_dir(export_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/path_helpers.py\", line 26, in get_or_create_variables_dir\n","    file_io.recursive_create_dir(variables_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 498, in recursive_create_dir\n","    recursive_create_dir_v2(dirname)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 513, in recursive_create_dir_v2\n","    _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: 231205_1430; Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-23-5ba1e85affc7\u003e\", line 1, in \u003ccell line: 1\u003e\n","    tf.saved_model.save(model,'231205_1430')\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py\", line 1331, in save\n","    save_and_return_nodes(obj, export_dir, signatures, options)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py\", line 1373, in save_and_return_nodes\n","    path_helpers.get_or_create_variables_dir(export_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/path_helpers.py\", line 26, in get_or_create_variables_dir\n","    file_io.recursive_create_dir(variables_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 498, in recursive_create_dir\n","    recursive_create_dir_v2(dirname)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 513, in recursive_create_dir_v2\n","    _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: 231205_1430; Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-23-5ba1e85affc7\u003e\", line 1, in \u003ccell line: 1\u003e\n","    tf.saved_model.save(model,'231205_1430')\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py\", line 1331, in save\n","    save_and_return_nodes(obj, export_dir, signatures, options)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py\", line 1373, in save_and_return_nodes\n","    path_helpers.get_or_create_variables_dir(export_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/path_helpers.py\", line 26, in get_or_create_variables_dir\n","    file_io.recursive_create_dir(variables_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 498, in recursive_create_dir\n","    recursive_create_dir_v2(dirname)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 513, in recursive_create_dir_v2\n","    _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: 231205_1430; Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["tf.saved_model.save(model,'231205_1430')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCRTtVGQJ3Xt"},"outputs":[],"source":["# Test Acc, Test Loss 확인\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Evaluate model\n","score = model.evaluate(X_test, y_test)\n","print(\"Test Loss:\", score[0])\n","\n","print(\"Test Accuracy:\", score[1])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxGOw8bSM8Xn"},"outputs":[],"source":["import time\n","\n","# Measure inference time for a single sample\n","start_time = time.time()\n","sample_prediction = model.predict(np.expand_dims(X_val[0], axis=0))  # Replace X_val[0] with your sample\n","end_time = time.time()\n","\n","inference_time = end_time - start_time\n","print(f\"Inference time for a single sample: {inference_time} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEYKjGcDOD4m"},"outputs":[],"source":["# Predict classes\n","y_pred = model.predict(X_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Find unique labels in test data\n","unique_labels = np.unique(y_test)\n","\n","# Create target names from unique labels\n","target_names = ['class_' + str(int(i)) for i in unique_labels]\n","\n","# Confusion Matrix 결과값\n","cm = confusion_matrix(y_test, y_pred_classes)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVrv51jDODwY"},"outputs":[],"source":["# Classification Report 분류확인 보고서\n","cr = classification_report(y_test, y_pred_classes, target_names=target_names)\n","print(\"Classification Report:\")\n","print(cr)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4123,"status":"ok","timestamp":1701753771175,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"LJT0ap8Yqq2-","outputId":"fcedd4e6-6ce4-4a51-a25b-680dac4c5b77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import re\n","import subprocess\n","from pathlib import Path\n","from typing import List, Optional\n","import openvino as ov\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tqdm import tqdm\n","import nncf\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive')\n","#print('current:', os.getcwd())\n","\n","ROOT = Path('/content/drive/MyDrive/231205_1430_model').parent.resolve()\n","WEIGHTS_URL = \"/content/drive/MyDrive/231205_1430.h5\"\n","DATASET_CLASSES = 8\n","\n","\n","def validate(model: ov.Model, val_loader: tf.data.Dataset) -\u003e tf.Tensor:\n","    compiled_model = ov.compile_model(model)\n","    output = compiled_model.outputs[0]\n","\n","    metric = tf.keras.metrics.CategoricalAccuracy(name=\"acc@1\")\n","    for images, labels in tqdm(val_loader):\n","        pred = compiled_model(images.numpy())[output]\n","        metric.update_state(labels, pred)\n","\n","    return metric.result()\n","\n","\n","def run_benchmark(model_path: str, shape: Optional[List[int]] = None, verbose: bool = True) -\u003e float:\n","    command = f\"benchmark_app -m {model_path} -d CPU -api async -t 15\"\n","    if shape is not None:\n","        command += f' -shape [{\",\".join(str(x) for x in shape)}]'\n","    cmd_output = subprocess.check_output(command, shell=True)  # nosec\n","    if verbose:\n","        print(*str(cmd_output).split(\"\\\\n\")[-9:-1], sep=\"\\n\")\n","    match = re.search(r\"Throughput\\: (.+?) FPS\", str(cmd_output))\n","    return float(match.group(1))\n","\n","\n","def get_model_size(ir_path: str, m_type: str = \"Mb\", verbose: bool = True) -\u003e float:\n","    xml_size = os.path.getsize(ir_path)\n","    bin_size = os.path.getsize(os.path.splitext(ir_path)[0] + \".bin\")\n","    for t in [\"bytes\", \"Kb\", \"Mb\"]:\n","        if m_type == t:\n","            break\n","        xml_size /= 1024\n","        bin_size /= 1024\n","    model_size = xml_size + bin_size\n","    if verbose:\n","        print(f\"Model graph (xml):   {xml_size:.3f} Mb\")\n","        print(f\"Model weights (bin): {bin_size:.3f} Mb\")\n","        print(f\"Model size:          {model_size:.3f} Mb\")\n","    return model_size\n","\n","\n","###############################################################################\n","# Create a Tensorflow model and dataset\n","\n","\n","def center_crop(image: tf.Tensor, image_size: int, crop_padding: int) -\u003e tf.Tensor:\n","    shape = tf.shape(image)\n","    image_height = shape[0]\n","    image_width = shape[1]\n","\n","    padded_center_crop_size = tf.cast(\n","        ((image_size / (image_size + crop_padding)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n","        tf.int32,\n","    )\n","\n","    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n","    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n","\n","    image = tf.image.crop_to_bounding_box(\n","        image,\n","        offset_height=offset_height,\n","        offset_width=offset_width,\n","        target_height=padded_center_crop_size,\n","        target_width=padded_center_crop_size,\n","    )\n","\n","    image = tf.compat.v1.image.resize(\n","        image, [image_size, image_size], method=tf.image.ResizeMethod.BILINEAR, align_corners=False\n","    )\n","\n","    return image\n","\n","\n","def preprocess_for_eval(image, label):\n","    image = center_crop(image, 224, 32)\n","    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","\n","    label = tf.one_hot(label, DATASET_CLASSES)\n","\n","    return image, label\n","\n","\n","val_dataset = tfds.load(\"imagenette/320px-v2\", split=\"validation\", shuffle_files=False, as_supervised=True)\n","val_dataset = val_dataset.map(preprocess_for_eval).batch(128)\n","\n","weights_path = tf.keras.utils.get_file(\"/content/drive/MyDrive/231205_1430\", WEIGHTS_URL, cache_subdir=\"models\")\n","tf_model = tf.keras.applications.MobileNetV2(weights=weights_path, classes=DATASET_CLASSES)\n","\n","###############################################################################\n","# Quantize a Tensorflow model\n","#\n","# The transformation function transforms a data item into model input data.\n","#\n","# To validate the transform function use the following code:\n","# \u003e\u003e for data_item in val_loader:\n","# \u003e\u003e    model(transform_fn(data_item))\n","\n","\n","# def transform_fn(data_item):\n","#     images, _ = data_item\n","#     return images\n","\n","\n","# # The calibration dataset is a small, no label, representative dataset\n","# # (~100-500 samples) that is used to estimate the range, i.e. (min, max) of all\n","# # floating point activation tensors in the model, to initialize the quantization\n","# # parameters.\n","# #\n","# # The easiest way to define a calibration dataset is to use a training or\n","# # validation dataset and a transformation function to remove labels from the data\n","# # item and prepare model input data. The quantize method uses a small subset\n","# # (default: 300 samples) of the calibration dataset.\n","\n","# calibration_dataset = nncf.Dataset(val_dataset, transform_fn)\n","# tf_quantized_model = nncf.quantize(tf_model, calibration_dataset)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":735172,"status":"ok","timestamp":1701753767066,"user":{"displayName":"MH Shin","userId":"11931192772106819334"},"user_tz":-540},"id":"hMigbT_-yLi4","outputId":"11f5821a-cf39-43d6-974a-e089f4c3c989"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:nncf:NNCF provides best results with tensorflow==2.12.*, while current tensorflow version is 2.14.0. If you encounter issues, consider switching to tensorflow==2.12.*\n","INFO:nncf:Creating compression algorithm: quantization\n","INFO:nncf:Overflow issue fix was applied to first convolution weight quantizers.\n","INFO:nncf:Collecting tensor statistics/data |█████           | 1 / 3\n","INFO:nncf:Collecting tensor statistics/data |██████████      | 2 / 3\n","INFO:nncf:Collecting tensor statistics/data |████████████████| 3 / 3\n","INFO:nncf:BatchNorm statistics adaptation |█████           | 1 / 3\n","INFO:nncf:BatchNorm statistics adaptation |██████████      | 2 / 3\n","INFO:nncf:BatchNorm statistics adaptation |████████████████| 3 / 3\n","[1/7] Save FP32 model: /content/drive/MyDrive/mobilenet_v2_fp32.xml\n","Model graph (xml):   0.079 Mb\n","Model weights (bin): 8.397 Mb\n","Model size:          8.476 Mb\n","[2/7] Save INT8 model: /content/drive/MyDrive/mobilenet_v2_int8.xml\n","Model graph (xml):   0.397 Mb\n","Model weights (bin): 2.242 Mb\n","Model size:          2.639 Mb\n","[3/7] Benchmark FP32 model:\n","[ INFO ] Count:            1452 iterations\n","[ INFO ] Duration:         15035.20 ms\n","[ INFO ] Latency:\n","[ INFO ]    Median:        18.21 ms\n","[ INFO ]    Average:       20.56 ms\n","[ INFO ]    Min:           15.30 ms\n","[ INFO ]    Max:           73.92 ms\n","[ INFO ] Throughput:   96.57 FPS\n","[4/7] Benchmark INT8 model:\n","[ INFO ] Count:            1658 iterations\n","[ INFO ] Duration:         15021.52 ms\n","[ INFO ] Latency:\n","[ INFO ]    Median:        15.84 ms\n","[ INFO ]    Average:       17.97 ms\n","[ INFO ]    Min:           14.37 ms\n","[ INFO ]    Max:           52.94 ms\n","[ INFO ] Throughput:   110.37 FPS\n","[5/7] Validate OpenVINO FP32 model:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 31/31 [01:21\u003c00:00,  2.65s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy @ top1: 0.198\n","[6/7] Validate OpenVINO INT8 model:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 31/31 [01:21\u003c00:00,  2.64s/it]"]},{"name":"stdout","output_type":"stream","text":["Accuracy @ top1: 0.198\n","[7/7] Report:\n","Accuracy drop: 0.000\n","Model compression rate: 3.211\n","Performance speed up (throughput mode): 1.143\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO:nncf:Creating compression algorithm: quantization\n","INFO:nncf:Overflow issue fix was applied to first convolution weight quantizers.\n","INFO:nncf:Collecting tensor statistics/data |█████           | 1 / 3\n","INFO:nncf:Collecting tensor statistics/data |██████████      | 2 / 3\n","INFO:nncf:Collecting tensor statistics/data |████████████████| 3 / 3\n","INFO:nncf:BatchNorm statistics adaptation |█████           | 1 / 3\n","INFO:nncf:BatchNorm statistics adaptation |██████████      | 2 / 3\n","INFO:nncf:BatchNorm statistics adaptation |████████████████| 3 / 3\n","[1/7] Save FP32 model: /content/drive/MyDrive/mobilenet_v2_fp32.xml\n","Model graph (xml):   0.079 Mb\n","Model weights (bin): 8.397 Mb\n","Model size:          8.476 Mb\n","[2/7] Save INT8 model: /content/drive/MyDrive/mobilenet_v2_int8.xml\n","Model graph (xml):   0.397 Mb\n","Model weights (bin): 2.242 Mb\n","Model size:          2.639 Mb\n","[3/7] Benchmark FP32 model:\n","[ INFO ] Count:            1302 iterations\n","[ INFO ] Duration:         15035.50 ms\n","[ INFO ] Latency:\n","[ INFO ]    Median:        18.28 ms\n","[ INFO ]    Average:       22.91 ms\n","[ INFO ]    Min:           12.46 ms\n","[ INFO ]    Max:           70.82 ms\n","[ INFO ] Throughput:   86.60 FPS\n","[4/7] Benchmark INT8 model:\n","[ INFO ] Count:            1632 iterations\n","[ INFO ] Duration:         15040.77 ms\n","[ INFO ] Latency:\n","[ INFO ]    Median:        16.01 ms\n","[ INFO ]    Average:       18.28 ms\n","[ INFO ]    Min:           13.75 ms\n","[ INFO ]    Max:           52.38 ms\n","[ INFO ] Throughput:   108.51 FPS\n","[5/7] Validate OpenVINO FP32 model:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 31/31 [01:22\u003c00:00,  2.65s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy @ top1: 0.198\n","[6/7] Validate OpenVINO INT8 model:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 31/31 [00:53\u003c00:00,  1.72s/it]"]},{"name":"stdout","output_type":"stream","text":["Accuracy @ top1: 0.198\n","[7/7] Report:\n","Accuracy drop: 0.000\n","Model compression rate: 3.211\n","Performance speed up (throughput mode): 1.253\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Quantize a Tensorflow model\n","#\n","# The transformation function transforms a data item into model input data.\n","#\n","# To validate the transform function use the following code:\n","# \u003e\u003e for data_item in val_loader:\n","# \u003e\u003e    model(transform_fn(data_item))\n","\n","\n","def transform_fn(data_item):\n","    images, _ = data_item\n","    return images\n","\n","\n","# The calibration dataset is a small, no label, representative dataset\n","# (~100-500 samples) that is used to estimate the range, i.e. (min, max) of all\n","# floating point activation tensors in the model, to initialize the quantization\n","# parameters.\n","#\n","# The easiest way to define a calibration dataset is to use a training or\n","# validation dataset and a transformation function to remove labels from the data\n","# item and prepare model input data. The quantize method uses a small subset\n","# (default: 300 samples) of the calibration dataset.\n","\n","calibration_dataset = nncf.Dataset(val_dataset, transform_fn)\n","tf_quantized_model = nncf.quantize(tf_model, calibration_dataset)\n","\n","###############################################################################\n","# Benchmark performance, calculate compression rate and validate accuracy\n","\n","ov_model = ov.convert_model(tf_model, share_weights=False)\n","#경량화 모델 : ov_quntized_model\n","ov_quantized_model = ov.convert_model(tf_quantized_model, share_weights=False)\n","\n","fp32_ir_path = f\"{ROOT}/mobilenet_v2_fp32.xml\"\n","ov.save_model(ov_model, fp32_ir_path, compress_to_fp16=False)\n","print(f\"[1/7] Save FP32 model: {fp32_ir_path}\")\n","fp32_model_size = get_model_size(fp32_ir_path, verbose=True)\n","\n","int8_ir_path = f\"{ROOT}/mobilenet_v2_int8.xml\"\n","ov.save_model(ov_quantized_model, int8_ir_path, compress_to_fp16=False)\n","print(f\"[2/7] Save INT8 model: {int8_ir_path}\")\n","int8_model_size = get_model_size(int8_ir_path, verbose=True)\n","\n","print(\"[3/7] Benchmark FP32 model:\")\n","fp32_fps = run_benchmark(fp32_ir_path, shape=[1, 224, 224, 3], verbose=True)\n","print(\"[4/7] Benchmark INT8 model:\")\n","int8_fps = run_benchmark(int8_ir_path, shape=[1, 224, 224, 3], verbose=True)\n","\n","print(\"[5/7] Validate OpenVINO FP32 model:\")\n","fp32_top1 = validate(ov_model, val_dataset)\n","print(f\"Accuracy @ top1: {fp32_top1:.3f}\")\n","\n","print(\"[6/7] Validate OpenVINO INT8 model:\")\n","int8_top1 = validate(ov_quantized_model, val_dataset)\n","print(f\"Accuracy @ top1: {int8_top1:.3f}\")\n","\n","print(\"[7/7] Report:\")\n","print(f\"Accuracy drop: {fp32_top1 - int8_top1:.3f}\")\n","print(f\"Model compression rate: {fp32_model_size / int8_model_size:.3f}\")\n","# https://docs.openvino.ai/latest/openvino_docs_optimization_guide_dldt_optimization_guide.html\n","print(f\"Performance speed up (throughput mode): {int8_fps / fp32_fps:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZgTYvuZX6cf"},"outputs":[],"source":["|"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}